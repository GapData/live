---
title: "Case study: wine quality"
author: "Mateusz Staniak"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r libs, echo = FALSE, warnin = FALSE, message = FALSE}
library(mlr)
library(live)
library(tidyverse)
library(bigstep)
```

### About the data 

Wine quality is a well-known dataset which was an object of study in Paulo Cortez, University of Minho, Guimarães, Portugal, http://www3.dsi.uminho.pt/pcortez A. Cerdeira, F. Almeida, T. Matos and J. Reis, Viticulture Commission of the Vinho Verde Region(CVRVV), Porto, Portugal 2009. Lichman, M. (2013).

The dataset contains variuos properties of a set of wines. For each wine there is 11 features based on physicochemical tests like density or pH and also and outcome variable: a score that describes the wine’s quality. The score is based on evaluations made by wine experts. 

According to the results from the original article, Support Vector Machine model is better than other models, including linear regression, for prediction purposes.
In this section we will show how `live` package can be used to fit linear regression model locally and generate a visual explanation for the black box model.

### Performance comparison

First, we can perform a quick bechmark experiment to see that SVM performs better than linear regression and, say, neural network.

| Learner | Mean MSE |
| ------- | -------- | 
| SVM | 0.39 |
| Linear regression | 0.43 | 
| Neural network | 0.55 | 

### Explaining single prediction

Suppose we want to understand prediction generated by SVM for the fifth observation in the dataset.
To do this, we need to generate observations for local exploration.

##### Simulating a dataset

We use `simulateSimilar` function from `live` package.
`standardise` is set to `TRUE`, so all variables will be normalized.
```{r similar, message = FALSE, warning = FALSE}
similar <- sample_locally(data = winequality_red,
                          explained_instance = winequality_red[5, ], 
                          explained_var = "quality", 
                          size = 100,
                          standardise = TRUE)
similar1 <- add_predictions(winequality_red, similar, "regr.randomForest")
similar2 <- add_predictions(winequality_red, similar, "regr.svm")
```

#### Training white box model

Then we fit linear regression model to predictions generated by SVM for this simulated dataset using \texttt{trainWhiteBox} function.
```{r training, warning = FALSE, message = FALSE, results='hide'}
trained <- fit_explanation(live_object = similar1,
                           white_box = "regr.lm",
                           selection = TRUE)
```

This function returns a native \texttt{mlr} object.
Model object (for example lm object) can be extracted using \texttt{getLearnerModel} function.
By setting \texttt{selection = TRUE} we choose to perform variable selection using AIC as implemented in \texttt{bigstep} package which is available on CRAN.

#### Model visualization

Now we can visualize fitted model by calling \texttt{plotWhiteBox} function.
```{r plot, message = FALSE, warning = FALSE}
plot_explanation(trained,
                 regr_plot_type = "forestplot",
                 explained_instance = winequality_red[5, ])
plot_explanation(trained,
                 regr_plot_type = "waterfall",
                 explained_instance = winequality_red[5, ])
```

Only variables chosen in the selection step are displayed.
They are sorted according to the absolute value of t-test statistic.
In the `Observed` column predictor values for case given in observation} argument are displayed.
`Estimate` column gives fitted parameter values, while `Lower` and `Upper` give lower and upper bounds of 95\% confidence interval, respectively.
The plot show `Estimate` value on x axis.
From the signs of the parameters and their magnitude wee can see how variables influence predictions.

